from transformers import pipeline
import torch
from typing import List, Dict, Tuple

class GemmaRecommender:
    def __init__(self):
        """Qwen2.5-1.5B-Instruct ëª¨ë¸ ì´ˆê¸°í™” (Pipeline ë°©ì‹)"""
        print("ğŸš€ Qwen2.5-1.5B-Instruct ëª¨ë¸ ë¡œë”© ì¤‘... (ìµœì´ˆ ì‹¤í–‰ì‹œ ë‹¤ìš´ë¡œë“œë¡œ ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
        try:
            # Qwen2.5-1.5B-Instruct Pipeline ë°©ì‹ (ì•ˆì •ì ì´ê³  í•œêµ­ì–´ ì§€ì› ìš°ìˆ˜)
            self.pipe = pipeline(
                "text-generation", 
                model="Qwen/Qwen2.5-1.5B-Instruct",
                device="cuda" if torch.cuda.is_available() else "cpu",
                torch_dtype=torch.bfloat16 if torch.cuda.is_available() else torch.float32
            )
            
            print("âœ… Qwen2.5-1.5B-Instruct ëª¨ë¸ ë¡œë”© ì™„ë£Œ!")
            print("ğŸ’¡ Qwen2.5ëŠ” 1.5B íŒŒë¼ë¯¸í„°ë¡œ ê°€ë³ê³  í•œêµ­ì–´ ì§€ì›ì´ ìš°ìˆ˜í•©ë‹ˆë‹¤")
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            print("ğŸ’¡ í•´ê²°ë°©ë²•:")
            print("   1. transformers ë²„ì „ í™•ì¸: pip install transformers>=4.50.0")
            print("   2. Hugging Face ë¡œê·¸ì¸: huggingface-cli login")
            print("   3. ë¼ì´ì„¼ìŠ¤ ë™ì˜: https://huggingface.co/Qwen/Qwen2.5-1.5B-Instruct")
            raise e
    
    def generate_recommendation_text(self, query: str, search_results: List[Tuple[Dict, float]]) -> str:
        """Qwen AIë¥¼ ì‚¬ìš©í•œ ìì—°ìŠ¤ëŸ¬ìš´ ì¶”ì²œ í…ìŠ¤íŠ¸ ìƒì„±"""
        if not search_results:
            return "ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        try:
            # ê²€ìƒ‰ ê²°ê³¼ ì •ë¦¬
            restaurants_info = []
            for i, (restaurant, score) in enumerate(search_results[:3], 1):
                restaurants_info.append(f"""
{i}. **{restaurant['name']}**
   - ìœ„ì¹˜: {restaurant['location']}
   - ë©”ë‰´: {restaurant['menu_type']}
   - ë¶„ìœ„ê¸°: {restaurant['atmosphere']}
   - ê°€ê²©ëŒ€: {restaurant['price_range']}
   - í‰ì : {restaurant['rating']}/5.0
   - í•œì¤„í‰: {restaurant['summary']}
""")
            
            restaurants_text = "\n".join(restaurants_info)
            
            # Qwen2.5ë¥¼ ìœ„í•œ ë©”ì‹œì§€ êµ¬ì„±
            messages = [
                {
                    "role": "system", 
                    "content": "ë‹¹ì‹ ì€ ì¹œê·¼í•˜ê³  ë„ì›€ì´ ë˜ëŠ” í•œêµ­ ë§›ì§‘ ì¶”ì²œ ì „ë¬¸ê°€ì…ë‹ˆë‹¤."
                },
                {
                    "role": "user", 
                    "content": f"""ì‚¬ìš©ì ìš”ì²­: "{query}"

ì¶”ì²œí•  ì‹ë‹¹ë“¤:
{restaurants_text}

ìœ„ ì‹ë‹¹ë“¤ì„ ì‚¬ìš©ì ìš”ì²­ì— ë§ê²Œ ë§¤ë ¥ì ìœ¼ë¡œ ì¶”ì²œí•´ì£¼ì„¸ìš”. ê° ì‹ë‹¹ì˜ íŠ¹ë³„í•œ ì ê³¼ ì¶”ì²œ ì´ìœ ë¥¼ í¬í•¨í•´ì„œ ìì—°ìŠ¤ëŸ½ê²Œ í•œêµ­ì–´ë¡œ ì„¤ëª…í•´ì£¼ì„¸ìš”. ê°„ê²°í•˜ê³  ì¹œê·¼í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”."""
                }
            ]
            
            # Pipelineì„ ì‚¬ìš©í•œ í…ìŠ¤íŠ¸ ìƒì„±
            outputs = self.pipe(
                messages, 
                max_new_tokens=1000,
                temperature=0.7,
                top_p=0.9,
                do_sample=True
            )
            
            # ì‘ë‹µ ì¶”ì¶œ
            if outputs and len(outputs) > 0:
                generated_text = outputs[0]["generated_text"]
                
                if isinstance(generated_text, list) and len(generated_text) > 2:
                    # ë§ˆì§€ë§‰ ë©”ì‹œì§€ (assistant ì‘ë‹µ) ì¶”ì¶œ
                    assistant_response = generated_text[-1]["content"].strip()
                    if assistant_response:
                        return f"ğŸ¤– **Qwen AI ì¶”ì²œ**\n\n{assistant_response}"
                elif isinstance(generated_text, str):
                    # ë¬¸ìì—´ì¸ ê²½ìš° ì›ë³¸ í”„ë¡¬í”„íŠ¸ ì´í›„ ë¶€ë¶„ ì¶”ì¶œ
                    prompt_end = generated_text.find("ê°„ê²°í•˜ê³  ì¹œê·¼í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”.")
                    if prompt_end != -1:
                        response = generated_text[prompt_end + len("ê°„ê²°í•˜ê³  ì¹œê·¼í•˜ê²Œ ì‘ì„±í•´ì£¼ì„¸ìš”."):].strip()
                        if response:
                            return f"ğŸ¤– **Qwen AI ì¶”ì²œ**\n\n{response}"
            
            # ì‘ë‹µì´ ì—†ìœ¼ë©´ í´ë°±
            return self._fallback_response(query, search_results)
            
        except Exception as e:
            print(f"AI ì¶”ì²œ ìƒì„± ì¤‘ ì˜¤ë¥˜: {e}")
            return self._fallback_response(query, search_results)
    
    def _fallback_response(self, query: str, search_results: List[Tuple[Dict, float]]) -> str:
        """AI ì‹¤íŒ¨ì‹œ í´ë°± ì‘ë‹µ"""
        simple_recommender = SimpleRecommender()
        return simple_recommender.generate_recommendation_text(query, search_results)

# í˜¸í™˜ì„±ì„ ìœ„í•œ ë³„ì¹­
LlamaRecommender = GemmaRecommender

# ê°„ë‹¨í•œ í…ŒìŠ¤íŠ¸ìš© í´ë˜ìŠ¤ (LLM ì—†ì´)
class SimpleRecommender:
    def generate_recommendation_text(self, user_query: str, search_results: List[Tuple[Dict, float]]) -> str:
        """ê°„ë‹¨í•œ í…œí”Œë¦¿ ê¸°ë°˜ ì¶”ì²œ"""
        if not search_results:
            return "ì£„ì†¡í•©ë‹ˆë‹¤. ì¡°ê±´ì— ë§ëŠ” ì‹ë‹¹ì„ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤."
        
        response = f"ğŸ” **'{user_query}'** ê²€ìƒ‰ ê²°ê³¼\n\n"
        
        for i, (restaurant, score) in enumerate(search_results, 1):
            response += f"""
**{i}. {restaurant['name']}** â­ {restaurant['rating']}/5.0
ğŸ“ {restaurant['location']} | ğŸ’° {restaurant['price_range']}
ğŸ½ï¸ {restaurant['menu_type']} | ğŸ­ {restaurant['atmosphere']}
ğŸ“ {restaurant['summary']}
ğŸ“Š ë§¤ì¹­ë„: {score:.1%}

"""
        
        response += "ğŸ’¡ **ë” ìì—°ìŠ¤ëŸ¬ìš´ AI ì¶”ì²œì„ ì›í•˜ì‹œë©´ ì‚¬ì´ë“œë°”ì—ì„œ 'Gemma AI ì¶”ì²œ ì‚¬ìš©'ì„ ì²´í¬í•´ë³´ì„¸ìš”!**"
        return response 